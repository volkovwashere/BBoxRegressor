{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('deep_learning': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8277c4497ec5ba7d6314982d6cc981d386d2f2b92a732db8e35c7edb9898c80e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "from cifar10_models import mobilenetv2\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training on device cuda.\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Net = mobilenetv2.mobilenet_v2(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for param in Net.parameters():\n",
    "#    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, pretrainedModel):\n",
    "        super().__init__()\n",
    "        self.features = pretrainedModel.features\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.Dense1 = nn.Linear(20480, 32)\n",
    "\n",
    "        self.outLayer1 = nn.Linear(32, 4)\n",
    "        self.outLayer2 = nn.Linear(32, 4)\n",
    "        self.outLayer3 = nn.Linear(32, 4)\n",
    "        self.outLayer4 = nn.Linear(32, 4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.Dense1(self.flatten(x))\n",
    "\n",
    "        out1 = self.outLayer1(x)\n",
    "        out2 = self.outLayer2(x)\n",
    "        out3 = self.outLayer3(x)\n",
    "        out4 = self.outLayer4(x)\n",
    "\n",
    "        return out1, out2, out3, out4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.CIFAR10('../data-unversioned/p1ch7/', train=True, transform=preprocess, download=False)\n",
    "imgs = os.listdir('data/training')\n",
    "imgs.sort()\n",
    "indices = [int(name[0:5]) for name in imgs]\n",
    "my_subset = Subset(dataset, indices) #create subset based on indices\n",
    "train_loader = DataLoader(my_subset, batch_size=1, shuffle=False) #shuffle false so we can use correct stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.from_numpy(np.load('data/labels.npy')).float()\n",
    "labels = labels.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(myset[1].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegressionModel(Net).to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-fa5483dd4abe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimgT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimgT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mout1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-743b66832454>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    415\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 416\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "img = cv2.imread('data/attacks/00000_input_img.png')\n",
    "imgT = torch.tensor(img)\n",
    "imgT = imgT.view(3, 32 , 32).to(device=device)\n",
    "imgT = torch.unsqueeze(imgT, 0).float()\n",
    "out1, out2, out3, out4 = model(imgT)\n",
    "out1[0].shape, out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'out1' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6bd3279f9544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out1' is not defined"
     ]
    }
   ],
   "source": [
    "loss(out1[0], labels[1][0])\n",
    "\n",
    "labels[1][0].dtype, out1[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, loader, labels):    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        loss_train = 0.0\n",
    "        idx = 0\n",
    "        for img_batch, _ in loader:\n",
    "            \n",
    "            img_batch = img_batch.to(device=device)\n",
    "            out1, out2, out3, out4 = model(img_batch)\n",
    "            loss1 = loss_fn(out1[0], labels[idx][0])\n",
    "            loss2 = loss_fn(out2[0], labels[idx][1])\n",
    "            loss3 = loss_fn(out3[0], labels[idx][2])\n",
    "            loss4 = loss_fn(out4[0], labels[idx][3])\n",
    "\n",
    "            loss_total = loss1 + loss2 + loss3 + loss4 #accumulate loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_total.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            idx += 1\n",
    "            \n",
    "            if idx % 100 == 0:\n",
    "                print(f'step {idx} is the current iteration')\n",
    "            \n",
    "        if epoch == 1 or epoch % 2 == 0:\n",
    "            print(f'At {epoch}, the training loss is {loss_total}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "step 100 is the current iteration\n",
      "step 200 is the current iteration\n",
      "step 300 is the current iteration\n",
      "step 400 is the current iteration\n",
      "step 500 is the current iteration\n",
      "step 600 is the current iteration\n",
      "step 700 is the current iteration\n",
      "step 800 is the current iteration\n",
      "step 900 is the current iteration\n",
      "step 1000 is the current iteration\n",
      "step 1100 is the current iteration\n",
      "step 1200 is the current iteration\n",
      "step 1300 is the current iteration\n",
      "step 1400 is the current iteration\n",
      "step 1500 is the current iteration\n",
      "step 1600 is the current iteration\n",
      "step 1700 is the current iteration\n",
      "step 1800 is the current iteration\n",
      "step 1900 is the current iteration\n",
      "step 2000 is the current iteration\n",
      "step 2100 is the current iteration\n",
      "step 2200 is the current iteration\n",
      "step 2300 is the current iteration\n",
      "step 2400 is the current iteration\n",
      "step 2500 is the current iteration\n",
      "step 2600 is the current iteration\n",
      "step 2700 is the current iteration\n",
      "step 2800 is the current iteration\n",
      "step 2900 is the current iteration\n",
      "step 3000 is the current iteration\n",
      "step 3100 is the current iteration\n",
      "step 3200 is the current iteration\n",
      "step 3300 is the current iteration\n",
      "step 3400 is the current iteration\n",
      "step 3500 is the current iteration\n",
      "step 3600 is the current iteration\n",
      "step 3700 is the current iteration\n",
      "step 3800 is the current iteration\n",
      "step 3900 is the current iteration\n",
      "step 4000 is the current iteration\n",
      "step 4100 is the current iteration\n",
      "step 4200 is the current iteration\n",
      "step 4300 is the current iteration\n",
      "step 4400 is the current iteration\n",
      "step 4500 is the current iteration\n",
      "step 4600 is the current iteration\n",
      "step 4700 is the current iteration\n",
      "At 1, the training loss is 104.68489074707031\n",
      "step 100 is the current iteration\n",
      "step 200 is the current iteration\n",
      "step 300 is the current iteration\n",
      "step 400 is the current iteration\n",
      "step 500 is the current iteration\n",
      "step 600 is the current iteration\n",
      "step 700 is the current iteration\n",
      "step 800 is the current iteration\n",
      "step 900 is the current iteration\n",
      "step 1000 is the current iteration\n",
      "step 1100 is the current iteration\n",
      "step 1200 is the current iteration\n",
      "step 1300 is the current iteration\n",
      "step 1400 is the current iteration\n",
      "step 1500 is the current iteration\n",
      "step 1600 is the current iteration\n",
      "step 1700 is the current iteration\n",
      "step 1800 is the current iteration\n",
      "step 1900 is the current iteration\n",
      "step 2000 is the current iteration\n",
      "step 2100 is the current iteration\n",
      "step 2200 is the current iteration\n",
      "step 2300 is the current iteration\n",
      "step 2400 is the current iteration\n",
      "step 2500 is the current iteration\n",
      "step 2600 is the current iteration\n",
      "step 2700 is the current iteration\n",
      "step 2800 is the current iteration\n",
      "step 2900 is the current iteration\n",
      "step 3000 is the current iteration\n",
      "step 3100 is the current iteration\n",
      "step 3200 is the current iteration\n",
      "step 3300 is the current iteration\n",
      "step 3400 is the current iteration\n",
      "step 3500 is the current iteration\n",
      "step 3600 is the current iteration\n",
      "step 3700 is the current iteration\n",
      "step 3800 is the current iteration\n",
      "step 3900 is the current iteration\n",
      "step 4000 is the current iteration\n",
      "step 4100 is the current iteration\n",
      "step 4200 is the current iteration\n",
      "step 4300 is the current iteration\n",
      "step 4400 is the current iteration\n",
      "step 4500 is the current iteration\n",
      "step 4600 is the current iteration\n",
      "step 4700 is the current iteration\n",
      "At 2, the training loss is 109.8509521484375\n",
      "step 100 is the current iteration\n",
      "step 200 is the current iteration\n",
      "step 300 is the current iteration\n",
      "step 400 is the current iteration\n",
      "step 500 is the current iteration\n",
      "step 600 is the current iteration\n",
      "step 700 is the current iteration\n",
      "step 800 is the current iteration\n",
      "step 900 is the current iteration\n",
      "step 1000 is the current iteration\n",
      "step 1100 is the current iteration\n",
      "step 1200 is the current iteration\n",
      "step 1300 is the current iteration\n",
      "step 1400 is the current iteration\n",
      "step 1500 is the current iteration\n",
      "step 1600 is the current iteration\n",
      "step 1700 is the current iteration\n",
      "step 1800 is the current iteration\n",
      "step 1900 is the current iteration\n",
      "step 2000 is the current iteration\n",
      "step 2100 is the current iteration\n",
      "step 2200 is the current iteration\n",
      "step 2300 is the current iteration\n",
      "step 2400 is the current iteration\n",
      "step 2500 is the current iteration\n",
      "step 2600 is the current iteration\n",
      "step 2700 is the current iteration\n",
      "step 2800 is the current iteration\n",
      "step 2900 is the current iteration\n",
      "step 3000 is the current iteration\n",
      "step 3100 is the current iteration\n",
      "step 3200 is the current iteration\n",
      "step 3300 is the current iteration\n",
      "step 3400 is the current iteration\n",
      "step 3500 is the current iteration\n",
      "step 3600 is the current iteration\n",
      "step 3700 is the current iteration\n",
      "step 3800 is the current iteration\n",
      "step 3900 is the current iteration\n",
      "step 4000 is the current iteration\n",
      "step 4100 is the current iteration\n",
      "step 4200 is the current iteration\n",
      "step 4300 is the current iteration\n",
      "step 4400 is the current iteration\n",
      "step 4500 is the current iteration\n",
      "step 4600 is the current iteration\n",
      "step 4700 is the current iteration\n",
      "step 100 is the current iteration\n",
      "step 200 is the current iteration\n",
      "step 300 is the current iteration\n",
      "step 400 is the current iteration\n",
      "step 500 is the current iteration\n",
      "step 600 is the current iteration\n",
      "step 700 is the current iteration\n",
      "step 800 is the current iteration\n",
      "step 900 is the current iteration\n",
      "step 1000 is the current iteration\n",
      "step 1100 is the current iteration\n",
      "step 1200 is the current iteration\n",
      "step 1300 is the current iteration\n",
      "step 1400 is the current iteration\n",
      "step 1500 is the current iteration\n",
      "step 1600 is the current iteration\n",
      "step 1700 is the current iteration\n",
      "step 1800 is the current iteration\n",
      "step 1900 is the current iteration\n",
      "step 2000 is the current iteration\n",
      "step 2100 is the current iteration\n",
      "step 2200 is the current iteration\n",
      "step 2300 is the current iteration\n",
      "step 2400 is the current iteration\n",
      "step 2500 is the current iteration\n",
      "step 2600 is the current iteration\n",
      "step 2700 is the current iteration\n",
      "step 2800 is the current iteration\n",
      "step 2900 is the current iteration\n",
      "step 3000 is the current iteration\n",
      "step 3100 is the current iteration\n",
      "step 3200 is the current iteration\n",
      "step 3300 is the current iteration\n",
      "step 3400 is the current iteration\n",
      "step 3500 is the current iteration\n",
      "step 3600 is the current iteration\n",
      "step 3700 is the current iteration\n",
      "step 3800 is the current iteration\n",
      "step 3900 is the current iteration\n",
      "step 4000 is the current iteration\n",
      "step 4100 is the current iteration\n",
      "step 4200 is the current iteration\n",
      "step 4300 is the current iteration\n",
      "step 4400 is the current iteration\n",
      "step 4500 is the current iteration\n",
      "step 4600 is the current iteration\n",
      "step 4700 is the current iteration\n",
      "At 4, the training loss is 108.71260833740234\n"
     ]
    }
   ],
   "source": [
    "training_loop(n_epochs=4, optimizer=optimizer, model=model, loss_fn=loss, loader=train_loader, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = my_subset[0][0].to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[14.3258, 14.8892,  2.7326,  2.7266]], device='cuda:0',\n",
       "        grad_fn=<AddmmBackward>),\n",
       " tensor([[12.7764, 13.5166,  3.9104,  3.8807]], device='cuda:0',\n",
       "        grad_fn=<AddmmBackward>),\n",
       " tensor([[14.4563, 15.2508,  3.1944,  3.6099]], device='cuda:0',\n",
       "        grad_fn=<AddmmBackward>),\n",
       " tensor([[15.3302, 13.4749,  4.2027,  4.3553]], device='cuda:0',\n",
       "        grad_fn=<AddmmBackward>))"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "model(torch.unsqueeze(asd, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[29.3383,  5.3111,  0.9465,  5.0000],\n",
       "        [ 5.0803, 13.2412,  4.5605,  2.1632],\n",
       "        [21.7605, 10.3194,  4.6590,  5.0000],\n",
       "        [13.8442, 19.0300,  5.0000,  5.0000]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}